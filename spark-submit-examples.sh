#!/bin/bash

# Example spark-submit commands for different scenarios

echo "=== Local Development ==="
echo "spark-submit \\"
echo "  --class com.example.streaming.KafkaSparkStreamingApp \\"
echo "  --master local[*] \\"
echo "  --packages io.delta:delta-core_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 \\"
echo "  target/kafka-spark-streaming-1.0-SNAPSHOT.jar"
echo ""

echo "=== With MinIO Configuration ==="
echo "spark-submit \\"
echo "  --class com.example.streaming.KafkaSparkStreamingApp \\"
echo "  --master local[*] \\"
echo "  --packages io.delta:delta-core_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 \\"
echo "  --conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \\"
echo "  --conf spark.hadoop.fs.s3a.access.key=minioadmin \\"
echo "  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \\"
echo "  --conf spark.hadoop.fs.s3a.path.style.access=true \\"
echo "  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\"
echo "  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \\"
echo "  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\"
echo "  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\"
echo "  --conf spark.databricks.delta.optimizeWrite.enabled=true \\"
echo "  --conf spark.databricks.delta.autoCompact.enabled=true \\"
echo "  target/kafka-spark-streaming-1.0-SNAPSHOT.jar"
echo ""

echo "=== AWS S3 Configuration ==="
echo "spark-submit \\"
echo "  --class com.example.streaming.KafkaSparkStreamingApp \\"
echo "  --master yarn \\"
echo "  --deploy-mode cluster \\"
echo "  --packages io.delta:delta-core_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 \\"
echo "  --conf spark.hadoop.fs.s3a.access.key=\$AWS_ACCESS_KEY_ID \\"
echo "  --conf spark.hadoop.fs.s3a.secret.key=\$AWS_SECRET_ACCESS_KEY \\"
echo "  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\"
echo "  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\"
echo "  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\"
echo "  --conf spark.databricks.delta.optimizeWrite.enabled=true \\"
echo "  --conf spark.databricks.delta.autoCompact.enabled=true \\"
echo "  target/kafka-spark-streaming-1.0-SNAPSHOT.jar"
echo ""

echo "=== Production Cluster ==="
echo "spark-submit \\"
echo "  --class com.example.streaming.KafkaSparkStreamingApp \\"
echo "  --master yarn \\"
echo "  --deploy-mode cluster \\"
echo "  --num-executors 4 \\"
echo "  --executor-memory 4g \\"
echo "  --executor-cores 2 \\"
echo "  --driver-memory 2g \\"
echo "  --packages io.delta:delta-core_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0 \\"
echo "  --conf spark.hadoop.fs.s3a.endpoint=\$S3_ENDPOINT \\"
echo "  --conf spark.hadoop.fs.s3a.access.key=\$S3_ACCESS_KEY \\"
echo "  --conf spark.hadoop.fs.s3a.secret.key=\$S3_SECRET_KEY \\"
echo "  --conf spark.hadoop.fs.s3a.path.style.access=true \\"
echo "  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\"
echo "  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\"
echo "  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\"
echo "  --conf spark.databricks.delta.optimizeWrite.enabled=true \\"
echo "  --conf spark.databricks.delta.autoCompact.enabled=true \\"
echo "  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\"
echo "  --conf spark.sql.adaptive.enabled=true \\"
echo "  --conf spark.sql.adaptive.coalescePartitions.enabled=true \\"
echo "  target/kafka-spark-streaming-1.0-SNAPSHOT.jar"
echo ""

echo "=== Environment Variables Required ==="
echo "export KAFKA_BROKERS=\"localhost:9092\""
echo "export KAFKA_TOPIC=\"messages\""
echo "export S3_ENDPOINT=\"http://localhost:9000\""
echo "export S3_ACCESS_KEY=\"minioadmin\""
echo "export S3_SECRET_KEY=\"minioadmin\""
echo "export S3_BUCKET=\"your-bucket\""